'''
å¯¹ RR ç¥ç»å…ƒæ„å»ºå›¾ï¼Œè¿›è¡Œç½‘ç»œåˆ†æ
'''
import os
import sys
import numpy as np
import matplotlib.pyplot as plt
import h5py
import igraph as ig
import seaborn as sns
import pandas as pd
from scipy import sparse
import warnings
import logging
from scipy.stats import zscore
from openpyxl import load_workbook, Workbook
from scipy.signal import butter, filtfilt
import scipy.io

warnings.filterwarnings('ignore')

# matplotlib å­—ä½“è®¾ç½®
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial']
plt.rcParams['axes.unicode_minus'] = False

# ==================== åˆ†ææ¨¡å¼é…ç½® ====================
# è®¾ç½®ä¸º True ä»…ä½¿ç”¨å…´å¥‹æ€§RRç¥ç»å…ƒï¼ŒFalse ä½¿ç”¨å…¨éƒ¨RRç¥ç»å…ƒ
USE_EXCITATORY_ONLY = True  # é»˜è®¤ä½¿ç”¨å…¨éƒ¨RRç¥ç»å…ƒ

# é€‰æ‹©åˆ†æçš„å°é¼ ï¼š'm79' æˆ– 'm21'
MOUSE_ID = 'm79'

# ==================== é…ç½®å‚æ•° ====================
# æ ¹æ®é€‰æ‹©çš„å°é¼ è®¾ç½®æ•°æ®è·¯å¾„
DATA_FILE = f"C:\\Users\\wangy\\Desktop\\IC\\{MOUSE_ID}\\wholebrain_output.mat"
RR_INDICES_CSV = f"C:\\Users\\wangy\\Desktop\\IC\\{MOUSE_ID}\\rr_neuron_original_indices.csv"

# æ»¤æ³¢å‚æ•°
SAMPLING_RATE = 4.0  # Hz
HIGH_PASS_CUTOFF = 0.05  # Hz

# é˜ˆå€¼æ‰«æå‚æ•°
SCAN_THRESHOLDS = np.arange(0.1, 0.5, 0.05)  # 0.1åˆ°0.5ï¼Œæ­¥é•¿0.05

# HubèŠ‚ç‚¹åˆ¤æ–­æ ‡å‡† - ç»Ÿä¸€ä½¿ç”¨z-score > 1.5
HUB_ZSCORE_THRESHOLD = 1.5

# ç®€åŒ–çš„æ—¥å¿—è®¾ç½® - åªæ˜¾ç¤ºä¿¡æ¯å†…å®¹
logging.basicConfig(level=logging.INFO,
                    format="%(message)s",  # åªæ˜¾ç¤ºä¿¡æ¯å†…å®¹
                    handlers=[logging.StreamHandler(sys.stdout)])
log = logging.getLogger(__name__)


# -------------------- æ»¤æ³¢å‡½æ•° --------------------
def high_pass_filter(data, cutoff, fs, order=4):
    """åº”ç”¨é«˜é€šæ»¤æ³¢å™¨"""
    nyquist = 0.5 * fs
    normal_cutoff = cutoff / nyquist
    b, a = butter(order, normal_cutoff, btype='high', analog=False)
    filtered_data = filtfilt(b, a, data, axis=1)
    return filtered_data


# -------------------- CSVè½¬ExcelåŠŸèƒ½ (ä¿®æ”¹ä¸ºä¿ç•™ä¸‰ä½æœ‰æ•ˆæ•°å­—) --------------------
def csv_to_excel(csv_path, excel_path=None):
    """å°†CSVæ–‡ä»¶è½¬æ¢ä¸ºæ ¼å¼åŒ–çš„Excelæ–‡ä»¶ï¼Œæ•°å€¼ä¿ç•™ä¸‰ä½æœ‰æ•ˆæ•°å­—"""
    if excel_path is None:
        excel_path = csv_path.replace('.csv', '_formatted.xlsx')
    
    try:
        # è¯»å– CSV
        df = pd.read_csv(csv_path)
        
        # å¤„ç†æ•°å€¼åˆ—ï¼Œä¿ç•™ä¸‰ä½æœ‰æ•ˆæ•°å­—
        for col in df.columns:
            if df[col].dtype in [np.float64, np.float32, np.int64, np.int32]:
                # å¯¹æ•°å€¼åˆ—åº”ç”¨æ ¼å¼åŒ–ï¼Œä¿ç•™ä¸‰ä½æœ‰æ•ˆæ•°å­—
                df[col] = df[col].apply(lambda x: float(f"{x:.3g}") if pd.notnull(x) and isinstance(x, (int, float)) else x)
        
        # å¦‚æœ Excel æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºä¸€ä¸ª
        try:
            wb = load_workbook(excel_path)
            ws = wb.active
        except:
            wb = Workbook()
            ws = wb.active
            ws.title = "Network Metrics"

        # æ¸…ç©ºå·²æœ‰å†…å®¹ï¼ˆé¿å…é‡å¤å†™å…¥ï¼‰
        ws.delete_rows(1, ws.max_row)

        # å†™å…¥è¡¨å¤´
        ws.append(list(df.columns))

        # å†™å…¥æ¯ä¸€è¡Œ
        for _, row in df.iterrows():
            # ä¿è¯åˆ—è¡¨ç±»æ•°æ®ä»¥å­—ç¬¦ä¸²å½¢å¼å†™å…¥
            clean_row = [str(x) if isinstance(x, (list, dict)) else x for x in row.values]
            ws.append(clean_row)

        # è‡ªé€‚åº”åˆ—å®½
        for column_cells in ws.columns:
            length = max(len(str(cell.value)) for cell in column_cells)
            ws.column_dimensions[column_cells[0].column_letter].width = min(length + 2, 50)

        # ä¿å­˜
        wb.save(excel_path)
        
        log.info(f"âœ… CSVè½¬Excelå®Œæˆï¼æ–‡ä»¶å·²ä¿å­˜åˆ°: {excel_path}")
        return excel_path
        
    except Exception as e:
        log.error(f"âŒ CSVè½¬Excelæ—¶å‡ºé”™: {e}")
        return None


# -------------------- æ•°æ®åŠ è½½ (ä¿®æ”¹ä¸ºæ”¯æŒä»…å…´å¥‹æ€§ç¥ç»å…ƒ) --------------------
def load_rr_neurons_data(file_path, rr_indices_csv_path, apply_filter=True):
    """ä»CSVæ–‡ä»¶åŠ è½½RRç¥ç»å…ƒçš„è§å…‰ä¿¡å·å’Œåæ ‡æ•°æ®"""
    rr_df = pd.read_csv(rr_indices_csv_path)
    
    # æ ¹æ®å¼€å…³é€‰æ‹©ç¥ç»å…ƒç±»å‹
    if USE_EXCITATORY_ONLY:
        # ä»…ä½¿ç”¨å…´å¥‹æ€§ç¥ç»å…ƒ
        rr_df = rr_df[rr_df['category'] == 'exc']
        log.info("ğŸ¯ åˆ†ææ¨¡å¼: ä»…ä½¿ç”¨å…´å¥‹æ€§RRç¥ç»å…ƒ")
    else:
        # ä½¿ç”¨å…¨éƒ¨RRç¥ç»å…ƒ
        log.info("ğŸ¯ åˆ†ææ¨¡å¼: ä½¿ç”¨å…¨éƒ¨RRç¥ç»å…ƒ (å…´å¥‹æ€§+æŠ‘åˆ¶æ€§)")
    
    rr_indices = rr_df['neuron_index'].values
    rr_categories = rr_df['category'].values
    
    log.info(f"åŠ è½½äº† {len(rr_indices)} ä¸ª RR ç¥ç»å…ƒç´¢å¼•")
    log.info(f"å…´å¥‹æ€§ç¥ç»å…ƒ: {np.sum(rr_categories == 'exc')} ä¸ª, æŠ‘åˆ¶æ€§ç¥ç»å…ƒ: {np.sum(rr_categories == 'inh')} ä¸ª")

    with h5py.File(file_path, 'r') as f:
        fluorescence = f['whole_trace_ori'][:]
        coordinates = f['whole_center'][:]

    fluorescence = fluorescence.T
    fluorescence_rr = fluorescence[rr_indices, :]

    # åº”ç”¨é«˜é€šæ»¤æ³¢
    if apply_filter:
        log.info(f"åº”ç”¨é«˜é€šæ»¤æ³¢: æˆªæ­¢é¢‘ç‡ {HIGH_PASS_CUTOFF} Hz, é‡‡æ ·ç‡ {SAMPLING_RATE} Hz")
        fluorescence_rr = high_pass_filter(fluorescence_rr, HIGH_PASS_CUTOFF, SAMPLING_RATE)
        log.info("âœ… é«˜é€šæ»¤æ³¢å®Œæˆ")

    coords_used = coordinates[:3, :] if coordinates.shape[0] >= 3 else coordinates
    coordinates_rr = coords_used[:, rr_indices].T

    log.info(f"RR è§å…‰å½¢çŠ¶: {fluorescence_rr.shape}, RR åæ ‡å½¢çŠ¶: {coordinates_rr.shape}")
    return fluorescence_rr, coordinates_rr, rr_indices, rr_categories


# ========== ä»RRåˆ†æåŠ è½½æ—¶é—´æ®µä¿¡æ¯ (ä¿®æ”¹ä¸ºæ”¯æŒ8ä¸ªæ—¶é—´æ®µ) ==========
def load_stimulus_periods_from_rr_analysis(data_path):
    """ä»RRåˆ†æä¿å­˜çš„æ–‡ä»¶ä¸­åŠ è½½8ä¸ªæ—¶é—´æ®µä¿¡æ¯"""
    try:
        # å°è¯•åŠ è½½npyæ–‡ä»¶ - ç°åœ¨æœ‰6ä¸ªåŸºç¡€æ—¶é—´æ®µ
        ic2_file = os.path.join(data_path, "stimulus_periods_ic2.npy")
        ic4_file = os.path.join(data_path, "stimulus_periods_ic4.npy")
        lc2_file = os.path.join(data_path, "stimulus_periods_lc2.npy")
        lc4_file = os.path.join(data_path, "stimulus_periods_lc4.npy")
        baseline_file = os.path.join(data_path, "stimulus_periods_baseline.npy")
        blank_screen_file = os.path.join(data_path, "stimulus_periods_blank_screen.npy")
        
        required_files = [ic2_file, ic4_file, lc2_file, lc4_file, baseline_file, blank_screen_file]
        
        if all(os.path.exists(f) for f in required_files):
            ic2_time_indices = np.load(ic2_file)
            ic4_time_indices = np.load(ic4_file)
            lc2_time_indices = np.load(lc2_file)
            lc4_time_indices = np.load(lc4_file)
            baseline_time_indices = np.load(baseline_file)
            blank_screen_indices = np.load(blank_screen_file)
            
            log.info(f"âœ… ä»npyæ–‡ä»¶åŠ è½½6ä¸ªåŸºç¡€æ—¶é—´æ®µä¿¡æ¯æˆåŠŸ")
            
            # åˆå¹¶IC2å’ŒIC4å¾—åˆ°IC
            ic_time_indices = np.concatenate([ic2_time_indices, ic4_time_indices])
            # åˆå¹¶LC2å’ŒLC4å¾—åˆ°LC
            lc_time_indices = np.concatenate([lc2_time_indices, lc4_time_indices])
            
            # éªŒè¯æ•°æ®æœ‰æ•ˆæ€§
            log.info(f"   åŸºç¡€æ—¶é—´æ®µç»Ÿè®¡:")
            log.info(f"   IC2: {len(ic2_time_indices)}ä¸ªæ—¶é—´ç‚¹")
            log.info(f"   IC4: {len(ic4_time_indices)}ä¸ªæ—¶é—´ç‚¹")
            log.info(f"   LC2: {len(lc2_time_indices)}ä¸ªæ—¶é—´ç‚¹")
            log.info(f"   LC4: {len(lc4_time_indices)}ä¸ªæ—¶é—´ç‚¹")
            log.info(f"   åŸºçº¿: {len(baseline_time_indices)}ä¸ªæ—¶é—´ç‚¹")
            log.info(f"   ç©ºç™½å±å¹•: {len(blank_screen_indices)}ä¸ªæ—¶é—´ç‚¹")
            log.info(f"   åˆå¹¶IC: {len(ic_time_indices)}ä¸ªæ—¶é—´ç‚¹")
            log.info(f"   åˆå¹¶LC: {len(lc_time_indices)}ä¸ªæ—¶é—´ç‚¹")
            
            return (ic_time_indices, lc_time_indices, 
                    ic2_time_indices, ic4_time_indices, 
                    lc2_time_indices, lc4_time_indices, 
                    baseline_time_indices, blank_screen_indices)
        else:
            missing_files = [os.path.basename(f) for f in required_files if not os.path.exists(f)]
            log.warning(f"âŒ æœªæ‰¾åˆ°å®Œæ•´çš„æ—¶é—´æ®µnpyæ–‡ä»¶ï¼Œç¼ºå¤±: {missing_files}")
            
            # å°è¯•åŠ è½½æ—§çš„å››ä¸ªæ—¶é—´æ®µæ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰
            return load_old_four_periods_format(data_path)
        
    except Exception as e:
        log.error(f"âŒ åŠ è½½æ—¶é—´æ®µä¿¡æ¯å¤±è´¥: {e}")
        return load_old_four_periods_format(data_path)


def load_old_four_periods_format(data_path):
    """åŠ è½½æ—§çš„å››ä¸ªæ—¶é—´æ®µæ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰"""
    try:
        # å°è¯•åŠ è½½æ—§çš„å››ä¸ªæ—¶é—´æ®µæ–‡ä»¶
        ic_file = os.path.join(data_path, "stimulus_periods_ic.npy")
        lc_file = os.path.join(data_path, "stimulus_periods_lc.npy")
        baseline_file = os.path.join(data_path, "stimulus_periods_baseline.npy")
        blank_screen_file = os.path.join(data_path, "stimulus_periods_blank_screen.npy")
        
        if all(os.path.exists(f) for f in [ic_file, lc_file, baseline_file, blank_screen_file]):
            ic_time_indices = np.load(ic_file)
            lc_time_indices = np.load(lc_file)
            baseline_time_indices = np.load(baseline_file)
            blank_screen_indices = np.load(blank_screen_file)
            
            log.info(f"âœ… ä»æ—§æ ¼å¼åŠ è½½4ä¸ªåŸºç¡€æ—¶é—´æ®µä¿¡æ¯æˆåŠŸ")
            log.info(f"   ICæ—¶é—´æ®µ: {len(ic_time_indices)}ä¸ªæ—¶é—´ç‚¹")
            log.info(f"   LCæ—¶é—´æ®µ: {len(lc_time_indices)}ä¸ªæ—¶é—´ç‚¹")
            log.info(f"   åŸºçº¿æ—¶é—´æ®µ: {len(baseline_time_indices)}ä¸ªæ—¶é—´ç‚¹")
            log.info(f"   ç©ºç™½å±å¹•æ—¶é—´æ®µ: {len(blank_screen_indices)}ä¸ªæ—¶é—´ç‚¹")
            
            # å¯¹äºæ—§æ ¼å¼ï¼Œå°†ICä½œä¸ºIC2å’ŒICçš„åˆå¹¶ï¼ŒLCä½œä¸ºLC2å’ŒLCçš„åˆå¹¶ï¼ŒIC4å’ŒLC4ä¸ºç©º
            return (ic_time_indices, lc_time_indices, 
                    ic_time_indices, np.array([]), 
                    lc_time_indices, np.array([]), 
                    baseline_time_indices, blank_screen_indices)
        else:
            log.warning("âŒ æœªæ‰¾åˆ°ä»»ä½•æ—¶é—´æ®µæ–‡ä»¶ï¼Œä½¿ç”¨å¤‡ç”¨åˆ†å‰²æ–¹æ³•")
            return (None, None, None, None, None, None, None, None)
    except Exception as e:
        log.error(f"âŒ åŠ è½½æ—§æ ¼å¼æ—¶é—´æ®µä¿¡æ¯å¤±è´¥: {e}")
        return (None, None, None, None, None, None, None, None)


# ========== ä½¿ç”¨RRåˆ†æä¿å­˜çš„æ—¶é—´æ®µåˆ†å‰²æ•°æ® (ä¿®æ”¹ä¸ºæ”¯æŒ8ä¸ªæ—¶é—´æ®µ) ==========
def split_data_into_eight_periods(fluorescence_rr, data_path):
    """ä½¿ç”¨RRåˆ†æä¿å­˜çš„æ—¶é—´æ®µä¿¡æ¯åˆ†å‰²æ•°æ®ä¸º8ä¸ªéƒ¨åˆ†"""
    # å°è¯•ä»RRåˆ†æä¿å­˜çš„æ–‡ä»¶åŠ è½½æ—¶é—´æ®µä¿¡æ¯
    (ic_time_indices, lc_time_indices, 
     ic2_time_indices, ic4_time_indices, 
     lc2_time_indices, lc4_time_indices, 
     baseline_time_indices, blank_screen_indices) = load_stimulus_periods_from_rr_analysis(data_path)
    
    if all(x is not None for x in [ic_time_indices, lc_time_indices, 
                                    ic2_time_indices, ic4_time_indices, 
                                    lc2_time_indices, lc4_time_indices, 
                                    baseline_time_indices, blank_screen_indices]):
        n_timepoints = fluorescence_rr.shape[1]
        
        # ç¡®ä¿æ—¶é—´ç‚¹ä¸è¶…å‡ºæ•°æ®èŒƒå›´
        ic_time_indices = [int(i) for i in ic_time_indices if i < n_timepoints]
        lc_time_indices = [int(i) for i in lc_time_indices if i < n_timepoints]
        ic2_time_indices = [int(i) for i in ic2_time_indices if i < n_timepoints]
        ic4_time_indices = [int(i) for i in ic4_time_indices if i < n_timepoints]
        lc2_time_indices = [int(i) for i in lc2_time_indices if i < n_timepoints]
        lc4_time_indices = [int(i) for i in lc4_time_indices if i < n_timepoints]
        baseline_time_indices = [int(i) for i in baseline_time_indices if i < n_timepoints]
        blank_screen_indices = [int(i) for i in blank_screen_indices if i < n_timepoints]
        
        ic_data = fluorescence_rr[:, ic_time_indices] if len(ic_time_indices) > 0 else np.empty((fluorescence_rr.shape[0], 0))
        lc_data = fluorescence_rr[:, lc_time_indices] if len(lc_time_indices) > 0 else np.empty((fluorescence_rr.shape[0], 0))
        ic2_data = fluorescence_rr[:, ic2_time_indices] if len(ic2_time_indices) > 0 else np.empty((fluorescence_rr.shape[0], 0))
        ic4_data = fluorescence_rr[:, ic4_time_indices] if len(ic4_time_indices) > 0 else np.empty((fluorescence_rr.shape[0], 0))
        lc2_data = fluorescence_rr[:, lc2_time_indices] if len(lc2_time_indices) > 0 else np.empty((fluorescence_rr.shape[0], 0))
        lc4_data = fluorescence_rr[:, lc4_time_indices] if len(lc4_time_indices) > 0 else np.empty((fluorescence_rr.shape[0], 0))
        baseline_data = fluorescence_rr[:, baseline_time_indices] if len(baseline_time_indices) > 0 else np.empty((fluorescence_rr.shape[0], 0))
        blank_screen_data = fluorescence_rr[:, blank_screen_indices] if len(blank_screen_indices) > 0 else np.empty((fluorescence_rr.shape[0], 0))
        
        log.info(f"âœ… ä½¿ç”¨RRåˆ†æä¿å­˜çš„æ—¶é—´æ®µåˆ†å‰²æ•°æ® (8ä¸ªæ—¶é—´æ®µ):")
        log.info(f"  åˆå¹¶IC: {ic_data.shape[1]}å¸§ (åŸå§‹: {len(ic_time_indices)}å¸§)")
        log.info(f"  åˆå¹¶LC: {lc_data.shape[1]}å¸§ (åŸå§‹: {len(lc_time_indices)}å¸§)")
        log.info(f"  IC2åˆºæ¿€: {ic2_data.shape[1]}å¸§ (åŸå§‹: {len(ic2_time_indices)}å¸§)")
        log.info(f"  IC4åˆºæ¿€: {ic4_data.shape[1]}å¸§ (åŸå§‹: {len(ic4_time_indices)}å¸§)")
        log.info(f"  LC2åˆºæ¿€: {lc2_data.shape[1]}å¸§ (åŸå§‹: {len(lc2_time_indices)}å¸§)")
        log.info(f"  LC4åˆºæ¿€: {lc4_data.shape[1]}å¸§ (åŸå§‹: {len(lc4_time_indices)}å¸§)")
        log.info(f"  åŸºçº¿: {baseline_data.shape[1]}å¸§ (åŸå§‹: {len(baseline_time_indices)}å¸§)")
        log.info(f"  ç©ºç™½å±å¹•: {blank_screen_data.shape[1]}å¸§ (åŸå§‹: {len(blank_screen_indices)}å¸§)")
        
        return ic_data, lc_data, ic2_data, ic4_data, lc2_data, lc4_data, baseline_data, blank_screen_data
    else:
        # å›é€€åˆ°é»˜è®¤åˆ†å‰²æ–¹æ³•
        log.warning("âŒ æ— æ³•åŠ è½½RRåˆ†æçš„æ—¶é—´æ®µä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤åˆ†å‰²æ–¹æ³•")
        return split_data_into_eight_periods_fallback(fluorescence_rr)


def split_data_into_eight_periods_fallback(fluorescence_rr):
    """é»˜è®¤çš„åˆ†å‰²æ–¹æ³•ï¼ˆå¤‡ç”¨ï¼‰"""
    n_timepoints = fluorescence_rr.shape[1]
    
    if n_timepoints < 8:
        # å¦‚æœæ•°æ®å¤ªå°‘ï¼Œå‡åŒ€åˆ†é…
        log.warning("æ•°æ®ç‚¹å¤ªå°‘ï¼Œä½¿ç”¨å‡åŒ€åˆ†é…")
        quarter = max(1, n_timepoints // 4)
        ic2_data = fluorescence_rr[:, :quarter]
        ic4_data = fluorescence_rr[:, quarter:2*quarter] if n_timepoints >= 2*quarter else np.empty((fluorescence_rr.shape[0], 0))
        lc2_data = fluorescence_rr[:, 2*quarter:3*quarter] if n_timepoints >= 3*quarter else np.empty((fluorescence_rr.shape[0], 0))
        lc4_data = fluorescence_rr[:, 3*quarter:] if n_timepoints >= 3*quarter else np.empty((fluorescence_rr.shape[0], 0))
        
        # åˆå¹¶ICå’ŒLC
        ic_data = np.concatenate([ic2_data, ic4_data], axis=1) if ic2_data.shape[1] > 0 or ic4_data.shape[1] > 0 else np.empty((fluorescence_rr.shape[0], 0))
        lc_data = np.concatenate([lc2_data, lc4_data], axis=1) if lc2_data.shape[1] > 0 or lc4_data.shape[1] > 0 else np.empty((fluorescence_rr.shape[0], 0))
        
        # åŸºçº¿å’Œç©ºç™½å±å¹•è®¾ä¸ºç©º
        baseline_data = np.empty((fluorescence_rr.shape[0], 0))
        blank_screen_data = np.empty((fluorescence_rr.shape[0], 0))
    else:
        # å…«ç­‰åˆ†åˆ†å‰²
        eighth = n_timepoints // 8
        
        ic2_data = fluorescence_rr[:, :eighth]
        ic4_data = fluorescence_rr[:, eighth:2*eighth] if n_timepoints >= 2*eighth else np.empty((fluorescence_rr.shape[0], 0))
        lc2_data = fluorescence_rr[:, 2*eighth:3*eighth] if n_timepoints >= 3*eighth else np.empty((fluorescence_rr.shape[0], 0))
        lc4_data = fluorescence_rr[:, 3*eighth:4*eighth] if n_timepoints >= 4*eighth else np.empty((fluorescence_rr.shape[0], 0))
        baseline_data = fluorescence_rr[:, 4*eighth:5*eighth] if n_timepoints >= 5*eighth else np.empty((fluorescence_rr.shape[0], 0))
        blank_screen_data = fluorescence_rr[:, 5*eighth:6*eighth] if n_timepoints >= 6*eighth else np.empty((fluorescence_rr.shape[0], 0))
        
        # åˆå¹¶ICå’ŒLC
        ic_data = np.concatenate([ic2_data, ic4_data], axis=1) if ic2_data.shape[1] > 0 or ic4_data.shape[1] > 0 else np.empty((fluorescence_rr.shape[0], 0))
        lc_data = np.concatenate([lc2_data, lc4_data], axis=1) if lc2_data.shape[1] > 0 or lc4_data.shape[1] > 0 else np.empty((fluorescence_rr.shape[0], 0))
    
    log.info(f"ä½¿ç”¨å…«ç­‰åˆ†åˆ†å‰²:")
    log.info(f"  åˆå¹¶IC: {ic_data.shape[1]}å¸§ (IC2: {ic2_data.shape[1]}å¸§, IC4: {ic4_data.shape[1]}å¸§)")
    log.info(f"  åˆå¹¶LC: {lc_data.shape[1]}å¸§ (LC2: {lc2_data.shape[1]}å¸§, LC4: {lc4_data.shape[1]}å¸§)")
    log.info(f"  åŸºçº¿: {baseline_data.shape[1]}å¸§")
    log.info(f"  ç©ºç™½å±å¹•: {blank_screen_data.shape[1]}å¸§")
    
    return ic_data, lc_data, ic2_data, ic4_data, lc2_data, lc4_data, baseline_data, blank_screen_data


# -------------------- ç›¸å…³æ€§è®¡ç®— --------------------
def calculate_correlation_matrix_rr(fluorescence_rr):
    """åŸºäºæ‰€æœ‰æ—¶é—´ç‚¹è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ"""
    n_neurons = fluorescence_rr.shape[0]
    log.info(f"è®¡ç®— {n_neurons} ä¸ª RR ç¥ç»å…ƒçš„ç›¸å…³æ€§çŸ©é˜µ...")

    means = np.mean(fluorescence_rr, axis=1, keepdims=True)
    stds = np.std(fluorescence_rr, axis=1, keepdims=True)
    stds[stds == 0] = 1.0

    fluorescence_normalized = (fluorescence_rr - means) / stds
    correlation_matrix = np.corrcoef(fluorescence_normalized)
    np.fill_diagonal(correlation_matrix, 0)
    return correlation_matrix


# -------------------- ç½‘ç»œæ„å»ºä¸é«˜çº§åˆ†æ --------------------
def create_network_analysis(correlation_matrix, threshold=0.2, rr_indices=None):
    """åˆ›å»ºç½‘ç»œå¹¶è®¡ç®—å¤šç§ç½‘ç»œæŒ‡æ ‡"""
    log.info(f"æ­£åœ¨åˆ›å»ºç½‘ç»œ (é˜ˆå€¼={threshold})...")
    mask = np.abs(correlation_matrix) >= threshold
    np.fill_diagonal(mask, False)

    sparse_matrix = sparse.coo_matrix(mask.astype(int))
    sources = sparse_matrix.row
    targets = sparse_matrix.col
    upper_triangular = sources < targets
    sources = sources[upper_triangular]
    targets = targets[upper_triangular]

    g = ig.Graph()
    n_nodes = correlation_matrix.shape[0]
    g.add_vertices(n_nodes)
    if len(sources) > 0:
        edges = list(zip(sources.tolist(), targets.tolist()))
        g.add_edges(edges)

    # åŸºç¡€ç½‘ç»œæŒ‡æ ‡
    degrees = np.array(g.degree())
    clustering_coeff = calculate_clustering_coefficient(g)
    connected_components = g.components()
    largest_component = connected_components.giant() if len(connected_components) > 0 else None
    
    # è®¡ç®—æ‰€æœ‰ç½‘ç»œæŒ‡æ ‡
    network_metrics = calculate_all_network_metrics(g, degrees, largest_component, rr_indices)
    
    return g, degrees, network_metrics


def calculate_clustering_coefficient(g):
    """è®¡ç®—èšç±»ç³»æ•°"""
    try:
        return g.transitivity_avglocal()
    except AttributeError:
        try:
            return g.transitivity_undirected()
        except AttributeError:
            return calculate_clustering_manual(g)


def calculate_clustering_manual(g):
    """æ‰‹åŠ¨è®¡ç®—èšç±»ç³»æ•°"""
    clustering_coeffs = []
    for node in range(g.vcount()):
        neighbors = g.neighbors(node)
        if len(neighbors) < 2:
            clustering_coeffs.append(0.0)
            continue
        
        neighbor_connections = 0
        for i in range(len(neighbors)):
            for j in range(i+1, len(neighbors)):
                if g.are_connected(neighbors[i], neighbors[j]):
                    neighbor_connections += 1
        
        possible_connections = len(neighbors) * (len(neighbors) - 1) / 2
        clustering_coeffs.append(neighbor_connections / possible_connections if possible_connections > 0 else 0.0)
    
    return np.mean(clustering_coeffs)


def calculate_all_network_metrics(g, degrees, largest_component, rr_indices):
    """è®¡ç®—æ‰€æœ‰ç½‘ç»œæŒ‡æ ‡"""
    metrics = {}
    
    # åŸºç¡€æŒ‡æ ‡
    metrics['n_nodes'] = g.vcount()
    metrics['n_edges'] = g.ecount()
    metrics['density'] = g.density()
    metrics['avg_degree'] = np.mean(degrees) if len(degrees) > 0 else 0
    metrics['max_degree'] = np.max(degrees) if len(degrees) > 0 else 0
    metrics['min_degree'] = np.min(degrees) if len(degrees) > 0 else 0
    metrics['clustering_coeff'] = calculate_clustering_coefficient(g)
    
    # è¿é€šæ€§æŒ‡æ ‡
    connected_components = g.components()
    metrics['n_components'] = len(connected_components)
    metrics['largest_component_size'] = largest_component.vcount() if largest_component else 0
    metrics['largest_component_ratio'] = metrics['largest_component_size'] / metrics['n_nodes'] if metrics['n_nodes'] > 0 else 0
    
    # è·¯å¾„é•¿åº¦æŒ‡æ ‡
    path_metrics = calculate_path_metrics(largest_component)
    metrics.update(path_metrics)
    
    # æ•ˆç‡æŒ‡æ ‡
    efficiency_metrics = calculate_efficiency_metrics(largest_component)
    metrics.update(efficiency_metrics)
    
    # ä¸­å¿ƒæ€§æŒ‡æ ‡
    centrality_metrics = calculate_centrality_metrics(g, degrees, largest_component, rr_indices)
    metrics.update(centrality_metrics)
    
    # æ¨¡å—åŒ–æŒ‡æ ‡
    modularity_metrics = calculate_modularity_metrics(g)
    metrics.update(modularity_metrics)
    
    # å°ä¸–ç•Œæ€§
    small_world_metrics = calculate_small_world_metrics(g)
    metrics.update(small_world_metrics)
    
    # åŒé…æ€§
    assortativity_metrics = calculate_assortativity_metrics(g)
    metrics.update(assortativity_metrics)
    
    # å¯Œä¿±ä¹éƒ¨ç³»æ•°
    rich_club_metrics = calculate_rich_club_metrics_manual(g, degrees)
    metrics.update(rich_club_metrics)
    
    return metrics


def calculate_path_metrics(largest_component):
    """è®¡ç®—è·¯å¾„ç›¸å…³æŒ‡æ ‡"""
    metrics = {}
    
    if largest_component and largest_component.vcount() > 1:
        try:
            # å¹³å‡æœ€çŸ­è·¯å¾„é•¿åº¦
            avg_path_length = np.mean(largest_component.shortest_paths())
            metrics['avg_path_length'] = avg_path_length
            
            # ç½‘ç»œç›´å¾„
            diameter = largest_component.diameter()
            metrics['diameter'] = diameter
            
        except Exception as e:
            log.warning(f"è®¡ç®—è·¯å¾„æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
            metrics['avg_path_length'] = float('inf')
            metrics['diameter'] = 0
    else:
        metrics['avg_path_length'] = float('inf')
        metrics['diameter'] = 0
    
    return metrics


def calculate_efficiency_metrics(largest_component):
    """è®¡ç®—æ•ˆç‡ç›¸å…³æŒ‡æ ‡"""
    metrics = {}
    
    if largest_component and largest_component.vcount() > 1:
        try:
            # å…¨å±€æ•ˆç‡
            distances = largest_component.shortest_paths()
            efficiencies = []
            for i in range(len(distances)):
                for j in range(i+1, len(distances)):
                    if distances[i][j] != float('inf') and distances[i][j] > 0:
                        efficiencies.append(1.0 / distances[i][j])
            
            metrics['global_efficiency'] = np.mean(efficiencies) if efficiencies else 0.0
            
            # å±€éƒ¨æ•ˆç‡ (ç®€åŒ–ç‰ˆæœ¬)
            local_efficiencies = []
            for node in range(largest_component.vcount()):
                neighbors = largest_component.neighbors(node)
                if len(neighbors) > 1:
                    subgraph = largest_component.induced_subgraph(neighbors)
                    if subgraph.vcount() > 1:
                        sub_distances = subgraph.shortest_paths()
                        sub_efficiencies = []
                        for i in range(len(sub_distances)):
                            for j in range(i+1, len(sub_distances)):
                                if sub_distances[i][j] != float('inf') and sub_distances[i][j] > 0:
                                    sub_efficiencies.append(1.0 / sub_distances[i][j])
                        if sub_efficiencies:
                            local_efficiencies.append(np.mean(sub_efficiencies))
            
            metrics['local_efficiency'] = np.mean(local_efficiencies) if local_efficiencies else 0.0
            
        except Exception as e:
            log.warning(f"è®¡ç®—æ•ˆç‡æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
            metrics['global_efficiency'] = 0.0
            metrics['local_efficiency'] = 0.0
    else:
        metrics['global_efficiency'] = 0.0
        metrics['local_efficiency'] = 0.0
    
    return metrics


def calculate_centrality_metrics(g, degrees, largest_component, rr_indices):
    """è®¡ç®—ä¸­å¿ƒæ€§ç›¸å…³æŒ‡æ ‡"""
    metrics = {}
    
    if g.vcount() == 0:
        return _get_empty_centrality_metrics()
    
    try:
        # åº¦ä¸­å¿ƒæ€§
        degree_centrality = np.array(degrees) / (g.vcount() - 1) if g.vcount() > 1 else np.zeros_like(degrees)
        metrics['avg_degree_centrality'] = float(np.mean(degree_centrality)) if len(degree_centrality) > 0 else 0.0
        
        # ä»‹æ•°ä¸­å¿ƒæ€§
        try:
            betweenness = g.betweenness()
            if isinstance(betweenness, list) and len(betweenness) > 0:
                betweenness_array = np.array(betweenness)
                metrics['avg_betweenness'] = float(np.mean(betweenness_array))
                metrics['max_betweenness'] = float(np.max(betweenness_array))
            else:
                metrics['avg_betweenness'] = 0.0
                metrics['max_betweenness'] = 0.0
        except Exception as e:
            log.warning(f"è®¡ç®—ä»‹æ•°ä¸­å¿ƒæ€§æ—¶å‡ºé”™: {e}")
            metrics['avg_betweenness'] = 0.0
            metrics['max_betweenness'] = 0.0
        
        # ç´§å¯†åº¦ä¸­å¿ƒæ€§ (åœ¨æœ€å¤§è¿é€šåˆ†é‡ä¸Šè®¡ç®—)
        try:
            if largest_component and largest_component.vcount() > 1:
                closeness = largest_component.closeness()
                if isinstance(closeness, list) and len(closeness) > 0:
                    closeness_array = np.array(closeness)
                    metrics['avg_closeness'] = float(np.mean(closeness_array))
                else:
                    metrics['avg_closeness'] = 0.0
            else:
                metrics['avg_closeness'] = 0.0
        except Exception as e:
            log.warning(f"è®¡ç®—ç´§å¯†åº¦ä¸­å¿ƒæ€§æ—¶å‡ºé”™: {e}")
            metrics['avg_closeness'] = 0.0
        
        # Hubåˆ†æ - ç»Ÿä¸€ä½¿ç”¨z-score > 1.5æ ‡å‡†
        if len(degrees) > 1:
            try:
                # ä½¿ç”¨ç»Ÿä¸€çš„z-scoreé˜ˆå€¼åˆ¤æ–­HubèŠ‚ç‚¹
                z_scores = zscore(degrees)
                hubs = np.where(z_scores > HUB_ZSCORE_THRESHOLD)[0]
                metrics['n_hubs'] = int(len(hubs))
                # ç§»é™¤äº† hub_fraction æŒ‡æ ‡
                
                # ä¿å­˜åŸå§‹æ•°æ®ä¸­çš„ç´¢å¼•å’Œæœ¬åœ°ç´¢å¼•
                if rr_indices is not None and len(hubs) > 0:
                    metrics['hub_indices_original'] = rr_indices[hubs].tolist()  # åœ¨å®Œæ•´æ•°æ®é›†ä¸­çš„åŸå§‹ç´¢å¼•
                    metrics['hub_indices_local'] = hubs.tolist()  # åœ¨RRå­é›†ä¸­çš„æœ¬åœ°ç´¢å¼•
                else:
                    metrics['hub_indices_original'] = []
                    metrics['hub_indices_local'] = hubs.tolist()
                
                metrics['hub_degrees'] = degrees[hubs].tolist() if len(hubs) > 0 else []
                
                # hub_z_scoresä¿ç•™ä¸‰ä½æœ‰æ•ˆæ•°å­—
                if len(hubs) > 0:
                    formatted_z_scores = [float(f"{score:.3g}") for score in z_scores[hubs]]
                    metrics['hub_z_scores'] = formatted_z_scores
                else:
                    metrics['hub_z_scores'] = []
                    
            except Exception as e:
                log.warning(f"è®¡ç®—HubæŒ‡æ ‡æ—¶å‡ºé”™: {e}")
                metrics.update(_get_empty_hub_metrics())
        else:
            metrics.update(_get_empty_hub_metrics())
                
    except Exception as e:
        log.warning(f"è®¡ç®—ä¸­å¿ƒæ€§æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
        metrics.update(_get_empty_centrality_metrics())
    
    return metrics


def _get_empty_centrality_metrics():
    """è¿”å›ç©ºçš„ä¸­å¿ƒæ€§æŒ‡æ ‡"""
    return {
        'avg_degree_centrality': 0.0,
        'avg_betweenness': 0.0,
        'max_betweenness': 0.0,
        'avg_closeness': 0.0
    }


def _get_empty_hub_metrics():
    """è¿”å›ç©ºçš„HubæŒ‡æ ‡"""
    return {
        'n_hubs': 0,
        'hub_indices_original': [],
        'hub_indices_local': [],
        'hub_degrees': [],
        'hub_z_scores': [],
    }


def calculate_modularity_metrics(g):
    """è®¡ç®—æ¨¡å—åŒ–ç›¸å…³æŒ‡æ ‡"""
    metrics = {}
    
    if g.vcount() > 1 and g.ecount() > 0:
        try:
            # ä½¿ç”¨Louvainç®—æ³•æ£€æµ‹ç¤¾åŒº
            communities = g.community_multilevel()
            modularity = g.modularity(communities)
            metrics['modularity'] = modularity
            metrics['n_communities'] = len(communities)
            metrics['avg_community_size'] = np.mean([len(c) for c in communities])
            
        except Exception as e:
            log.warning(f"è®¡ç®—æ¨¡å—åŒ–æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
            metrics['modularity'] = 0
            metrics['n_communities'] = 1
            metrics['avg_community_size'] = g.vcount()
    else:
        metrics['modularity'] = 0
        metrics['n_communities'] = 1
        metrics['avg_community_size'] = g.vcount() if g.vcount() > 0 else 0
    
    return metrics


def calculate_small_world_metrics(g):
    """è®¡ç®—å°ä¸–ç•Œæ€§ç›¸å…³æŒ‡æ ‡"""
    metrics = {}
    
    if g.vcount() < 10:  # ç½‘ç»œå¤ªå°æ— æ³•å¯é è®¡ç®—
        metrics['small_worldness'] = 0.0
        return metrics
    
    try:
        # è®¡ç®—å®é™…ç½‘ç»œçš„èšç±»ç³»æ•°å’Œå¹³å‡è·¯å¾„é•¿åº¦
        C_real = calculate_clustering_coefficient(g)
        
        # è®¡ç®—å®é™…ç½‘ç»œçš„å¹³å‡è·¯å¾„é•¿åº¦ï¼ˆä½¿ç”¨æœ€å¤§è¿é€šåˆ†é‡ï¼‰
        giant = g.components().giant()
        if giant.vcount() < 2:
            metrics['small_worldness'] = 0.0
            return metrics
        
        L_real = np.mean(giant.shortest_paths())
        
        # ç”Ÿæˆéšæœºç½‘ç»œå¹¶è®¡ç®—å¹³å‡å€¼
        n_random = 3  # å‡å°‘éšæœºç½‘ç»œæ•°é‡ä»¥æé«˜é€Ÿåº¦
        C_random_list = []
        L_random_list = []
        
        for _ in range(n_random):
            # ç”Ÿæˆç›¸åŒèŠ‚ç‚¹æ•°å’Œè¾¹æ•°çš„éšæœºç½‘ç»œ
            random_g = ig.Graph.Erdos_Renyi(n=g.vcount(), m=g.ecount())
            C_random_list.append(calculate_clustering_coefficient(random_g))
            
            # è®¡ç®—éšæœºç½‘ç»œçš„å¹³å‡è·¯å¾„é•¿åº¦
            random_giant = random_g.components().giant()
            if random_giant.vcount() > 1:
                try:
                    L_random = np.mean(random_giant.shortest_paths())
                    L_random_list.append(L_random)
                except:
                    pass
        
        C_random = np.mean(C_random_list) if C_random_list else 1.0
        L_random = np.mean(L_random_list) if L_random_list else 1.0
        
        # å°ä¸–ç•Œæ€§ = (C_real / C_random) / (L_real / L_random)
        if C_random > 0 and L_random > 0 and L_real > 0:
            small_worldness = (C_real / C_random) / (L_real / L_random)
            metrics['small_worldness'] = small_worldness
        else:
            metrics['small_worldness'] = 0.0
            
    except Exception as e:
        log.warning(f"è®¡ç®—å°ä¸–ç•Œæ€§æ—¶å‡ºé”™: {e}")
        metrics['small_worldness'] = 0.0
    
    return metrics


def calculate_assortativity_metrics(g):
    """è®¡ç®—åŒé…æ€§ç›¸å…³æŒ‡æ ‡"""
    metrics = {}
    
    if g.vcount() > 1 and g.ecount() > 0:
        try:
            # åº¦åŒé…æ€§
            assortativity = g.assortativity_degree()
            metrics['assortativity'] = assortativity
            
        except Exception as e:
            log.warning(f"è®¡ç®—åŒé…æ€§æ—¶å‡ºé”™: {e}")
            metrics['assortativity'] = 0.0
    else:
        metrics['assortativity'] = 0.0
    
    return metrics


def calculate_rich_club_metrics_manual(g, degrees):
    """æ‰‹åŠ¨è®¡ç®—å¯Œä¿±ä¹éƒ¨ç³»æ•°"""
    metrics = {}
    
    if g.vcount() < 3:
        metrics['avg_rich_club'] = 0.0
        metrics['max_rich_club'] = 0.0
        return metrics
    
    try:
        # å¯Œä¿±ä¹éƒ¨ç³»æ•°å®šä¹‰ï¼šå¯¹äºåº¦æ•°ä¸ºkçš„èŠ‚ç‚¹ï¼Œè®¡ç®—è¿™äº›é«˜åº¦æ•°èŠ‚ç‚¹ä¹‹é—´å®é™…è¾¹æ•°ä¸å¯èƒ½æœ€å¤§è¾¹æ•°çš„æ¯”ä¾‹
        max_degree = int(np.max(degrees))
        
        # åªè®¡ç®—åˆ°æœ€å¤§åº¦æ•°çš„ä¸€åŠï¼Œé¿å…ç»Ÿè®¡ä¸å¯é 
        max_k = min(max_degree // 2, 10)
        
        if max_k < 1:
            metrics['avg_rich_club'] = 0.0
            metrics['max_rich_club'] = 0.0
            return metrics
        
        rich_club_coeffs = []
        
        for k in range(1, max_k + 1):
            # æ‰¾å‡ºåº¦æ•°å¤§äºkçš„èŠ‚ç‚¹
            high_degree_nodes = [i for i, deg in enumerate(degrees) if deg > k]
            n_high = len(high_degree_nodes)
            
            if n_high < 2:
                continue
            
            # è®¡ç®—è¿™äº›é«˜åº¦æ•°èŠ‚ç‚¹ä¹‹é—´çš„å®é™…è¾¹æ•°
            actual_edges = 0
            for i in range(n_high):
                for j in range(i + 1, n_high):
                    if g.are_connected(high_degree_nodes[i], high_degree_nodes[j]):
                        actual_edges += 1
            
            # å¯èƒ½çš„è¾¹æ•°
            possible_edges = n_high * (n_high - 1) / 2
            
            if possible_edges > 0:
                rich_club_coeff = actual_edges / possible_edges
                rich_club_coeffs.append(rich_club_coeff)
        
        if rich_club_coeffs:
            metrics['avg_rich_club'] = float(np.mean(rich_club_coeffs))
            metrics['max_rich_club'] = float(np.max(rich_club_coeffs))
        else:
            metrics['avg_rich_club'] = 0.0
            metrics['max_rich_club'] = 0.0
            
    except Exception as e:
        log.warning(f"è®¡ç®—å¯Œä¿±ä¹éƒ¨ç³»æ•°æ—¶å‡ºé”™: {e}")
        metrics['avg_rich_club'] = 0.0
        metrics['max_rich_club'] = 0.0
    
    return metrics


def print_network_metrics(metrics, rr_categories, threshold, stimulus_type="All"):
    """æ‰“å°æ‰€æœ‰ç½‘ç»œæŒ‡æ ‡"""
    log.info("\n" + "="*70)
    log.info(f"            RRç¥ç»å…ƒç½‘ç»œåˆ†æç»“æœ (åˆºæ¿€ç±»å‹: {stimulus_type}, é˜ˆå€¼={threshold})")
    log.info("="*70)
    
    # åŸºç¡€ä¿¡æ¯
    log.info(f"\nğŸ“Š åŸºç¡€ä¿¡æ¯:")
    log.info(f"   â€¢ ç¥ç»å…ƒæ€»æ•°: {metrics['n_nodes']}")
    log.info(f"   â€¢ å…´å¥‹æ€§ç¥ç»å…ƒ: {np.sum(rr_categories == 'exc')}")
    log.info(f"   â€¢ æŠ‘åˆ¶æ€§ç¥ç»å…ƒ: {np.sum(rr_categories == 'inh')}")
    
    # ç½‘ç»œç»“æ„æŒ‡æ ‡
    log.info(f"\nğŸ—ï¸  ç½‘ç»œç»“æ„:")
    log.info(f"   â€¢ è¾¹æ•°: {metrics['n_edges']}")
    log.info(f"   â€¢ ç½‘ç»œå¯†åº¦: {metrics['density']:.3g}")
    log.info(f"   â€¢ å¹³å‡åº¦æ•°: {metrics['avg_degree']:.3g}")
    log.info(f"   â€¢ æœ€å¤§åº¦æ•°: {metrics['max_degree']}")
    log.info(f"   â€¢ è¿é€šåˆ†é‡æ•°: {metrics['n_components']}")
    log.info(f"   â€¢ æœ€å¤§è¿é€šåˆ†é‡æ¯”ä¾‹: {metrics['largest_component_ratio']:.3g}")
    
    # èšç±»å’Œè·¯å¾„æŒ‡æ ‡
    log.info(f"\nğŸ”— æ‹“æ‰‘ç‰¹æ€§:")
    log.info(f"   â€¢ å¹³å‡èšç±»ç³»æ•°: {metrics['clustering_coeff']:.3g}")
    log.info(f"   â€¢ å¹³å‡æœ€çŸ­è·¯å¾„: {metrics['avg_path_length']:.3g}")
    log.info(f"   â€¢ ç½‘ç»œç›´å¾„: {metrics['diameter']}")
    
    # æ•ˆç‡æŒ‡æ ‡
    log.info(f"\nâš¡ æ•ˆç‡æŒ‡æ ‡:")
    log.info(f"   â€¢ å…¨å±€æ•ˆç‡: {metrics['global_efficiency']:.3g}")
    log.info(f"   â€¢ å±€éƒ¨æ•ˆç‡: {metrics['local_efficiency']:.3g}")
    
    # ä¸­å¿ƒæ€§æŒ‡æ ‡
    log.info(f"\nğŸ¯ ä¸­å¿ƒæ€§åˆ†æ:")
    log.info(f"   â€¢ å¹³å‡åº¦ä¸­å¿ƒæ€§: {metrics['avg_degree_centrality']:.3g}")
    log.info(f"   â€¢ å¹³å‡ä»‹æ•°ä¸­å¿ƒæ€§: {metrics['avg_betweenness']:.3g}")
    log.info(f"   â€¢ å¹³å‡ç´§å¯†åº¦ä¸­å¿ƒæ€§: {metrics['avg_closeness']:.3g}")
    
    # Hubåˆ†æ - ç§»é™¤äº†hub_fraction
    log.info(f"\nğŸ¯ HubèŠ‚ç‚¹åˆ†æ (z-score > {HUB_ZSCORE_THRESHOLD}):")
    log.info(f"   â€¢ HubèŠ‚ç‚¹æ•°é‡: {metrics['n_hubs']}ä¸ª")
    
    if metrics['n_hubs'] > 0:
        # æ˜¾ç¤ºåŸå§‹ç´¢å¼•ï¼ˆåœ¨å®Œæ•´æ•°æ®é›†ä¸­çš„ç´¢å¼•ï¼‰
        if len(metrics['hub_indices_original']) > 0:
            log.info(f"   â€¢ HubèŠ‚ç‚¹åŸå§‹ç´¢å¼•: {metrics['hub_indices_original']}")
        # æ˜¾ç¤ºæœ¬åœ°ç´¢å¼•ï¼ˆåœ¨RRå­é›†ä¸­çš„ç´¢å¼•ï¼‰
        if len(metrics['hub_indices_local']) > 0:
            log.info(f"   â€¢ HubèŠ‚ç‚¹æœ¬åœ°ç´¢å¼•: {metrics['hub_indices_local']}")
        
        if len(metrics['hub_degrees']) > 0:
            log.info(f"   â€¢ HubèŠ‚ç‚¹åº¦æ•°èŒƒå›´: {np.min(metrics['hub_degrees'])} - {np.max(metrics['hub_degrees'])}")
        if len(metrics['hub_z_scores']) > 0:
            min_z = float(f"{np.min(metrics['hub_z_scores']):.3g}")
            max_z = float(f"{np.max(metrics['hub_z_scores']):.3g}")
            log.info(f"   â€¢ HubèŠ‚ç‚¹z-scoreèŒƒå›´: {min_z:.3g} - {max_z:.3g}")
    
    # æ¨¡å—åŒ–æŒ‡æ ‡
    log.info(f"\nğŸ§© æ¨¡å—åŒ–åˆ†æ:")
    log.info(f"   â€¢ æ¨¡å—åº¦: {metrics['modularity']:.3g}")
    log.info(f"   â€¢ ç¤¾åŒºæ•°é‡: {metrics['n_communities']}")
    log.info(f"   â€¢ å¹³å‡ç¤¾åŒºå¤§å°: {metrics['avg_community_size']:.3g}")
    
    # é«˜çº§ç½‘ç»œç‰¹æ€§
    log.info(f"\nğŸŒŸ é«˜çº§ç½‘ç»œç‰¹æ€§:")
    log.info(f"   â€¢ å°ä¸–ç•Œæ€§: {metrics['small_worldness']:.3g}")
    log.info(f"   â€¢ åŒé…æ€§: {metrics['assortativity']:.3g}")
    log.info(f"   â€¢ å¹³å‡å¯Œä¿±ä¹éƒ¨ç³»æ•°: {metrics['avg_rich_club']:.3g}")
    log.info(f"   â€¢ æœ€å¤§å¯Œä¿±ä¹éƒ¨ç³»æ•°: {metrics['max_rich_club']:.3g}")
    
    log.info("="*70)


# -------------------- é˜ˆå€¼æ‰«æåŠŸèƒ½ --------------------
def threshold_scan_analysis(fluorescence_rr, rr_categories, rr_indices, stimulus_type="All"):
    """æ‰§è¡Œçš®å°”é€Šç³»æ•°é˜ˆå€¼æ‰«æåˆ†æ"""
    log.info("\n" + "="*70)
    log.info(f"             å¼€å§‹çš®å°”é€Šç³»æ•°é˜ˆå€¼æ‰«æåˆ†æ (åˆºæ¿€ç±»å‹: {stimulus_type})")
    log.info("="*70)
    
    # ä½¿ç”¨é…ç½®çš„é˜ˆå€¼èŒƒå›´
    thresholds = SCAN_THRESHOLDS
    all_metrics = []
    
    # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µï¼ˆåªéœ€è®¡ç®—ä¸€æ¬¡ï¼‰
    log.info("è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ...")
    correlation_matrix = calculate_correlation_matrix_rr(fluorescence_rr)
    
    for threshold in thresholds:
        log.info(f"\n>>> æ­£åœ¨åˆ†æé˜ˆå€¼: {threshold:.2f}")
        
        try:
            # ç½‘ç»œåˆ†æï¼ˆä¼ é€’rr_indicesç”¨äºHubç´¢å¼•æ˜ å°„ï¼‰
            g, degrees, network_metrics = create_network_analysis(correlation_matrix, threshold, rr_indices)
            
            # æ·»åŠ é˜ˆå€¼ä¿¡æ¯
            network_metrics['threshold'] = threshold
            network_metrics['stimulus_type'] = stimulus_type
            
            # æ·»åŠ ç¥ç»å…ƒç±»å‹ä¿¡æ¯
            network_metrics['n_exc_neurons'] = np.sum(rr_categories == 'exc')
            network_metrics['n_inh_neurons'] = np.sum(rr_categories == 'inh')
            
            all_metrics.append(network_metrics)
            
            # æ‰“å°å½“å‰é˜ˆå€¼çš„ç»“æœæ‘˜è¦
            log.info(f"   èŠ‚ç‚¹æ•°: {network_metrics['n_nodes']}, è¾¹æ•°: {network_metrics['n_edges']}, "
                   f"å¯†åº¦: {network_metrics['density']:.3g}, HubèŠ‚ç‚¹: {network_metrics['n_hubs']}ä¸ª")
            
        except Exception as e:
            log.error(f"é˜ˆå€¼ {threshold} åˆ†æå¤±è´¥: {e}")
            continue
    
    # ä¿å­˜ç»“æœåˆ°DataFrameä½†ä¸ä¿å­˜æ–‡ä»¶
    if all_metrics:
        df = pd.DataFrame(all_metrics)
        
        # é‡æ–°æ’åˆ—åˆ—çš„é¡ºåºï¼Œè®©thresholdå’Œstimulus_typeåœ¨å‰
        cols = ['threshold', 'stimulus_type'] + [col for col in df.columns if col not in ['threshold', 'stimulus_type']]
        df = df[cols]
        
        log.info(f"\nâœ… {stimulus_type}é˜ˆå€¼æ‰«æå®Œæˆï¼")
        log.info(f"   å…±åˆ†æäº† {len(all_metrics)} ä¸ªé˜ˆå€¼")
        
        # æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
        log.info(f"\nğŸ“ˆ {stimulus_type}é˜ˆå€¼æ‰«æç»Ÿè®¡æ‘˜è¦:")
        log.info(f"   é˜ˆå€¼èŒƒå›´: {thresholds[0]:.2f} - {thresholds[-1]:.2f}")
        log.info(f"   è¾¹æ•°èŒƒå›´: {df['n_edges'].min()} - {df['n_edges'].max()}")
        log.info(f"   å¯†åº¦èŒƒå›´: {df['density'].min():.3g} - {df['density'].max():.3g}")
        log.info(f"   HubèŠ‚ç‚¹æ•°é‡èŒƒå›´: {df['n_hubs'].min()} - {df['n_hubs'].max()}")
        
        return df
    else:
        log.error(f"âŒ {stimulus_type}æ²¡æœ‰æˆåŠŸåˆ†æä»»ä½•é˜ˆå€¼")
        return None


# -------------------- å¯¹æ¯”åˆ†æåŠŸèƒ½ (ä¿®æ”¹ä¸ºæ”¯æŒ8ä¸ªæ—¶é—´æ®µ) --------------------
def compare_stimulus_periods(all_results, output_dir):
    """å¯¹æ¯”8ä¸ªæ—¶é—´æ®µï¼ˆICã€LCã€IC2ã€IC4ã€LC2ã€LC4ã€åŸºçº¿ã€ç©ºç™½å±å¹•ï¼‰çš„ç½‘ç»œæŒ‡æ ‡ï¼ŒåŒ…å«æ‰€æœ‰é˜ˆå€¼ç»“æœ"""
    log.info("\n" + "="*70)
    log.info("             å¼€å§‹8ä¸ªæ—¶é—´æ®µçš„ç½‘ç»œæŒ‡æ ‡å¯¹æ¯” (åŒ…å«æ‰€æœ‰é˜ˆå€¼)")
    log.info("="*70)
    
    if not all_results:
        log.warning("âŒ æ²¡æœ‰å¯å¯¹æ¯”çš„ç»“æœæ•°æ®")
        return
    
    # æ”¶é›†æ‰€æœ‰æ—¶é—´æ®µçš„æ‰€æœ‰é˜ˆå€¼ç»“æœ
    comparison_data = []
    
    for stimulus_type, results in all_results.items():
        if results is not None:
            # åŒ…å«æ‰€æœ‰é˜ˆå€¼çš„ç»“æœ
            for _, row in results.iterrows():
                metrics = row.to_dict()
                metrics['stimulus_type'] = stimulus_type
                comparison_data.append(metrics)
    
    if len(comparison_data) < 2:
        log.warning("âŒ å¯å¯¹æ¯”çš„æ•°æ®æ•°é‡ä¸è¶³")
        return
    
    # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
    df_comparison = pd.DataFrame(comparison_data)
    
    # ç¡®ä¿é˜ˆå€¼åˆ—åœ¨æœ€å‰é¢
    if 'threshold' in df_comparison.columns:
        cols = ['threshold', 'stimulus_type'] + [col for col in df_comparison.columns if col not in ['threshold', 'stimulus_type']]
        df_comparison = df_comparison[cols]
    
    # é€‰æ‹©å…³é”®æŒ‡æ ‡è¿›è¡Œå¯¹æ¯”ï¼ˆç§»é™¤äº†hub_fractionï¼‰
    key_metrics = [
        'n_nodes', 'n_edges', 'density', 'avg_degree', 'clustering_coeff',
        'avg_path_length', 'global_efficiency', 'local_efficiency',
        'modularity', 'small_worldness', 'assortativity', 'n_hubs'
    ]
    
    # è¿‡æ»¤å‡ºå­˜åœ¨çš„æŒ‡æ ‡
    available_metrics = [metric for metric in key_metrics if metric in df_comparison.columns]
    
    # ä¿å­˜å®Œæ•´çš„å¯¹æ¯”ç»“æœï¼ˆåŒ…å«æ‰€æœ‰é˜ˆå€¼ï¼‰
    comparison_csv = os.path.join(output_dir, "network_comparison_eight_periods_all_thresholds.csv")
    df_comparison.to_csv(comparison_csv, index=False, encoding='utf-8-sig')
    
    log.info(f"\nğŸ“Š 8ä¸ªæ—¶é—´æ®µç½‘ç»œæŒ‡æ ‡å¯¹æ¯” (æ‰€æœ‰é˜ˆå€¼):")
    
    # å®šä¹‰æ—¶é—´æ®µæ˜¾ç¤ºé¡ºåº
    stimulus_order = ['IC', 'LC', 'IC2', 'IC4', 'LC2', 'LC4', 'Baseline', 'Blank_Screen']
    
    for stimulus_type in stimulus_order:
        if stimulus_type in df_comparison['stimulus_type'].unique():
            subset = df_comparison[df_comparison['stimulus_type'] == stimulus_type]
            log.info(f"\nğŸ¯ {stimulus_type}åˆºæ¿€æœŸé—´ (é˜ˆå€¼èŒƒå›´: {subset['threshold'].min():.2f}-{subset['threshold'].max():.2f}):")
            log.info(f"   èŠ‚ç‚¹æ•°: {subset['n_nodes'].mean():.1f}Â±{subset['n_nodes'].std():.1f}")
            log.info(f"   è¾¹æ•°: {subset['n_edges'].mean():.1f}Â±{subset['n_edges'].std():.1f}")
            log.info(f"   å¯†åº¦: {subset['density'].mean():.3g}Â±{subset['density'].std():.3g}")
            log.info(f"   èšç±»ç³»æ•°: {subset['clustering_coeff'].mean():.3g}Â±{subset['clustering_coeff'].std():.3g}")
            log.info(f"   HubèŠ‚ç‚¹: {subset['n_hubs'].mean():.1f}Â±{subset['n_hubs'].std():.1f}ä¸ª")

    # è‡ªåŠ¨è½¬æ¢ä¸ºExcelæ ¼å¼ï¼ˆä¿ç•™ä¸‰ä½æœ‰æ•ˆæ•°å­—ï¼‰
    comparison_excel = os.path.join(output_dir, "network_comparison_eight_periods_all_thresholds_formatted.xlsx")
    csv_to_excel(comparison_csv, comparison_excel)
    
    log.info(f"\nâœ… 8ä¸ªæ—¶é—´æ®µå¯¹æ¯”åˆ†æå®Œæˆï¼")
    log.info(f"   â€¢ å®Œæ•´å¯¹æ¯”è¡¨æ ¼CSV: {comparison_csv}")
    log.info(f"   â€¢ å®Œæ•´å¯¹æ¯”è¡¨æ ¼Excel: {comparison_excel}")
    
    return df_comparison


# -------------------- åˆ†åˆºæ¿€ç±»å‹åˆ†æ (ä¿®æ”¹ä¸ºæ”¯æŒ8ä¸ªæ—¶é—´æ®µ) --------------------
def analyze_by_stimulus_type(fluorescence_rr, rr_categories, rr_indices, output_dir, data_path):
    """æŒ‰åˆºæ¿€ç±»å‹åˆ†åˆ«è¿›è¡Œåˆ†æï¼ˆICã€LCã€IC2ã€IC4ã€LC2ã€LC4ã€åŸºçº¿ã€ç©ºç™½å±å¹•ï¼‰ï¼Œåªç”Ÿæˆå¯¹æ¯”ç»“æœæ–‡ä»¶"""
    log.info("\n" + "="*70)
    log.info("             å¼€å§‹æŒ‰åˆºæ¿€ç±»å‹åˆ†åˆ«åˆ†æ (ICã€LCã€IC2ã€IC4ã€LC2ã€LC4ã€åŸºçº¿ã€ç©ºç™½å±å¹•)")
    log.info("="*70)
    
    # ä½¿ç”¨RRåˆ†æä¿å­˜çš„æ—¶é—´æ®µä¿¡æ¯åˆ†å‰²æ•°æ®ï¼ˆ8ä¸ªæ—¶é—´æ®µï¼‰
    ic_data, lc_data, ic2_data, ic4_data, lc2_data, lc4_data, baseline_data, blank_screen_data = split_data_into_eight_periods(fluorescence_rr, data_path)
    
    all_results = {}
    
    # åˆ†æ8ä¸ªæ—¶é—´æ®µ
    periods = [
        ("IC", ic_data),
        ("LC", lc_data),
        ("IC2", ic2_data),
        ("IC4", ic4_data),
        ("LC2", lc2_data), 
        ("LC4", lc4_data),
        ("Baseline", baseline_data),
        ("Blank_Screen", blank_screen_data)
    ]
    
    for stimulus_type, data in periods:
        if data.shape[1] > 0:
            log.info(f"\nğŸ”Š åˆ†æ{stimulus_type}æœŸé—´æ•°æ® (æ—¶é—´ç‚¹: {data.shape[1]})")
            results = threshold_scan_analysis(data, rr_categories, rr_indices, stimulus_type)
            all_results[stimulus_type] = results
        else:
            log.warning(f"âŒ {stimulus_type}æœŸé—´æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡åˆ†æ")
    
    # æ‰§è¡Œå¯¹æ¯”åˆ†æ
    comparison_results = compare_stimulus_periods(all_results, output_dir)
    
    return all_results, comparison_results


# -------------------- ä¸»æµç¨‹ (ä¿®æ”¹ä¸ºæ”¯æŒ8ä¸ªæ—¶é—´æ®µ) --------------------
def main():
    try:
        log.info("=== RR ç¥ç»å…ƒç½‘ç»œåˆ†æå¼€å§‹ ===")
        
        # æ˜¾ç¤ºå½“å‰æ¨¡å¼é…ç½®
        log.info(f"ğŸ­ åˆ†æå°é¼ : {MOUSE_ID}")
        if USE_EXCITATORY_ONLY:
            log.info(f"ğŸ¯ ç¥ç»å…ƒé€‰æ‹©: ä»…å…´å¥‹æ€§RRç¥ç»å…ƒ")
        else:
            log.info(f"ğŸ¯ ç¥ç»å…ƒé€‰æ‹©: å…¨éƒ¨RRç¥ç»å…ƒ (å…´å¥‹æ€§+æŠ‘åˆ¶æ€§)")
        
        log.info(f"ğŸ“Š åˆ†ææ¨¡å¼: é˜ˆå€¼æ‰«æ (èŒƒå›´: {SCAN_THRESHOLDS[0]:.2f} - {SCAN_THRESHOLDS[-1]:.2f})")
        log.info(f"ğŸ¯ HubèŠ‚ç‚¹åˆ¤æ–­æ ‡å‡†: z-score > {HUB_ZSCORE_THRESHOLD}")
        log.info(f"ğŸ”Š åˆºæ¿€ç±»å‹åˆ†æ: å¼€å¯ (ICã€LCã€IC2ã€IC4ã€LC2ã€LC4ã€åŸºçº¿ã€ç©ºç™½å±å¹•)")
        log.info(f"ğŸ“ˆ é«˜é€šæ»¤æ³¢: æˆªæ­¢é¢‘ç‡ {HIGH_PASS_CUTOFF} Hz, é‡‡æ ·ç‡ {SAMPLING_RATE} Hz")
        log.info(f"ğŸ“Š æ•°å€¼ç²¾åº¦: ä¿ç•™ä¸‰ä½æœ‰æ•ˆæ•°å­—")
        
        # åŠ è½½æ•°æ®ï¼ˆåº”ç”¨é«˜é€šæ»¤æ³¢ï¼‰
        fluorescence_rr, coordinates_rr, rr_indices, rr_categories = load_rr_neurons_data(DATA_FILE, RR_INDICES_CSV, apply_filter=True)
        
        # ç¡®å®šè¾“å‡ºç›®å½•å’Œæ•°æ®ç›®å½•
        output_dir = os.path.dirname(DATA_FILE)
        data_path = output_dir  # æ•°æ®ç›®å½•ä¸è¾“å‡ºç›®å½•ç›¸åŒ
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨RRåˆ†æçš„æ—¶é—´æ®µæ–‡ä»¶
        required_files = [
            "stimulus_periods_ic2.npy",
            "stimulus_periods_ic4.npy",
            "stimulus_periods_lc2.npy",
            "stimulus_periods_lc4.npy",
            "stimulus_periods_baseline.npy",
            "stimulus_periods_blank_screen.npy"
        ]
        
        existing_files = [f for f in required_files if os.path.exists(os.path.join(data_path, f))]
        
        if len(existing_files) == len(required_files):
            log.info("âœ… æ£€æµ‹åˆ°å®Œæ•´çš„RRåˆ†ææ—¶é—´æ®µæ–‡ä»¶ï¼Œå°†ä½¿ç”¨ç²¾ç¡®çš„æ—¶é—´æ®µåˆ†å‰²")
        elif len(existing_files) > 0:
            log.warning(f"âš ï¸  æ‰¾åˆ°éƒ¨åˆ†æ—¶é—´æ®µæ–‡ä»¶ ({len(existing_files)}/{len(required_files)})ï¼Œå°†ä½¿ç”¨å¯ç”¨æ—¶é—´æ®µ")
        else:
            log.warning("âš ï¸  æœªæ‰¾åˆ°RRåˆ†ææ—¶é—´æ®µæ–‡ä»¶ï¼Œå°†ä½¿ç”¨é»˜è®¤åˆ†å‰²æ–¹æ³•")
        
        # æŒ‰åˆºæ¿€ç±»å‹åˆ†åˆ«åˆ†æï¼ˆICã€LCã€IC2ã€IC4ã€LC2ã€LC4ã€åŸºçº¿ã€ç©ºç™½å±å¹•ï¼‰
        all_results, comparison_results = analyze_by_stimulus_type(fluorescence_rr, rr_categories, rr_indices, output_dir, data_path)
        
        log.info(f"\nğŸ“Š æ‰€æœ‰åˆºæ¿€ç±»å‹åˆ†æå®Œæˆ!")
        for stimulus_type, results in all_results.items():
            if results is not None:
                log.info(f"   â€¢ {stimulus_type}: åˆ†æå®Œæˆ")
        
        log.info("\nğŸ‰ æ‰€æœ‰åˆ†æå®Œæˆ!")

    except Exception as e:
        log.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()